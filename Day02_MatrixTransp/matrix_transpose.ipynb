{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N92Ia264UQDZ"
      },
      "source": [
        "# Day 2: Matrix Transpose (Coalesced vs Non-Coalesced)\n",
        "\n",
        "In this notebook, we compare the performance of a matrix transpose operation implemented using two different CUDA kernels: one with non-coalesced access and one optimized for coalesced access using shared memory. We'll compile and run the CUDA code on Google Colab's T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdmYM-T1UQDb",
        "outputId": "02a38778-ee4d-4d2b-d2e2-064de084ce70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_transpose.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matrix_transpose.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "#define TILE_DIM 32\n",
        "#define BLOCK_ROWS 8\n",
        "\n",
        "// Non-coalesced Matrix Transpose Kernel\n",
        "__global__ void transposeNonCoalesced(const float *in, float *out, int width, int height) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < height && col < width) {\n",
        "        // Writing to the output in a way that results in non-coalesced accesses\n",
        "        out[col * height + row] = in[row * width + col];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Coalesced Matrix Transpose Kernel using shared memory\n",
        "__global__ void transposeCoalesced(const float *in, float *out, int width, int height) {\n",
        "    // Shared memory tile with an extra column to avoid bank conflicts\n",
        "    __shared__ float tile[TILE_DIM][TILE_DIM+1];\n",
        "\n",
        "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "\n",
        "    if (x < width && y < height) {\n",
        "        tile[threadIdx.y][threadIdx.x] = in[y * width + x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Compute transposed indices\n",
        "    x = blockIdx.y * TILE_DIM + threadIdx.x; // swap blockIdx.x and blockIdx.y\n",
        "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "    if (x < height && y < width) {\n",
        "        out[y * height + x] = tile[threadIdx.x][threadIdx.y];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    // Get matrix dimensions from command-line arguments\n",
        "    int width = 1024;  // Default width\n",
        "    int height = 1024; // Default height\n",
        "\n",
        "    if (argc > 2) {\n",
        "        width = atoi(argv[1]);\n",
        "        height = atoi(argv[2]);\n",
        "    }\n",
        "    int size = width * height;\n",
        "    size_t bytes = size * sizeof(float);\n",
        "\n",
        "    // Allocate host memory\n",
        "    float *h_in = (float*)malloc(bytes);\n",
        "    float *h_out = (float*)malloc(bytes);\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for(int i = 0; i < size; i++) {\n",
        "        h_in[i] = static_cast<float>(i);\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_in, *d_out;\n",
        "    cudaMalloc(&d_in, bytes);\n",
        "    cudaMalloc(&d_out, bytes);\n",
        "\n",
        "    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configure grid and block dimensions\n",
        "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
        "    dim3 grid((width + TILE_DIM - 1) / TILE_DIM, (height + TILE_DIM - 1) / TILE_DIM);\n",
        "\n",
        "    // Run Non-Coalesced Transpose Kernel\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    transposeNonCoalesced<<<grid, block>>>(d_in, d_out, width, height);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> duration_non_coalesced = end - start;\n",
        "\n",
        "    // Run Coalesced Transpose Kernel\n",
        "    start = std::chrono::high_resolution_clock::now();\n",
        "    transposeCoalesced<<<grid, block>>>(d_in, d_out, width, height);\n",
        "    cudaDeviceSynchronize();\n",
        "    end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> duration_coalesced = end - start;\n",
        "\n",
        "    // Print the results\n",
        "    std::cout << \"Non-Coalesced Time: \" << duration_non_coalesced.count() << \" s\" << std::endl;\n",
        "    std::cout << \"Coalesced Time: \" << duration_coalesced.count() << \" s\" << std::endl;\n",
        "\n",
        "    // Free allocated memory\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    free(h_in);\n",
        "    free(h_out);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1dolAwSUQDc"
      },
      "outputs": [],
      "source": [
        "!nvcc matrix_transpose.cu -o matrix_transpose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function to run the CUDA code and get execution times\n",
        "def run_transpose_cuda(width, height):\n",
        "    command = [\"./matrix_transpose\", str(width), str(height)]\n",
        "    output = subprocess.check_output(command).decode(\"utf-8\")\n",
        "    lines = output.split(\"\\n\")\n",
        "    non_coalesced_time = float(lines[0].split(\":\")[1].strip().split(\" \")[0])\n",
        "    coalesced_time = float(lines[1].split(\":\")[1].strip().split(\" \")[0])\n",
        "    return non_coalesced_time, coalesced_time\n",
        "\n",
        "# Function to perform CPU-based matrix transpose\n",
        "def transpose_cpu(matrix):\n",
        "    start_time = time.time()\n",
        "    transposed_matrix = matrix.T  # Using NumPy's transpose function\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# Matrix dimensions to test\n",
        "dimensions = [(256, 256), (512, 512), (1024, 1024), (2048, 2048)]\n",
        "\n",
        "# Store execution times\n",
        "non_coalesced_times = []\n",
        "coalesced_times = []\n",
        "cpu_times = []\n",
        "\n",
        "# Run the transpose for different matrix sizes\n",
        "for width, height in dimensions:\n",
        "    # CUDA transpose\n",
        "    non_coalesced_time, coalesced_time = run_transpose_cuda(width, height)\n",
        "    non_coalesced_times.append(non_coalesced_time)\n",
        "    coalesced_times.append(coalesced_time)\n",
        "\n",
        "    # CPU transpose\n",
        "    matrix = np.random.rand(width, height)  # Create a random matrix\n",
        "    cpu_time = transpose_cpu(matrix)\n",
        "    cpu_times.append(cpu_time)\n",
        "\n",
        "# Extract widths for plotting\n",
        "widths = [dim[0] for dim in dimensions]\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(widths, non_coalesced_times, label=\"Non-Coalesced (CUDA)\")\n",
        "plt.plot(widths, coalesced_times, label=\"Coalesced (CUDA)\")\n",
        "plt.plot(widths, cpu_times, label=\"CPU\")\n",
        "plt.xlabel(\"Matrix Dimension\")\n",
        "plt.ylabel(\"Log of Execution Time (s)\")\n",
        "plt.title(\"Coalesced vs Non-Coalesced vs CPU Matrix Transpose\")\n",
        "plt.legend()\n",
        "\n",
        "# Adjust y-axis scale\n",
        "plt.yscale(\"log\")  # Change to logarithmic scale\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCup4t93UYkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QNU6RIIUQDc",
        "outputId": "f5ac7ab6-bc21-4adb-9128-1c352ef7e32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-Coalesced Time: 0.00778203 s\n",
            "Coalesced Time: 2.153e-06 s\n"
          ]
        }
      ],
      "source": [
        "!./matrix_transpose 128 256  # Example: width = 128, height = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qs7lhoqVYkmS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}